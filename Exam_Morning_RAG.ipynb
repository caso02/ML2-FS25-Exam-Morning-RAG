{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf56921",
   "metadata": {},
   "source": [
    "# Exam (morning): Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7fb22f",
   "metadata": {},
   "source": [
    "### Personal Details (please complete)\n",
    "Double Click on Cell to edit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d35a25",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td>First Name:</td>\n",
    "    <td>Lucas</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Last Name:</td>\n",
    "    <td>Hersche</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Student ID:</td>\n",
    "    <td>1408236</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Modul:</td>\n",
    "    <td>Machine Learning 2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Exam Date / Raum / Zeit:</td>\n",
    "    <td>20.05.2025 / Raum: SM O2.01  / 10:15 – 11:30</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Erlaubte Hilfsmittel:</td>\n",
    "    <td>w.3ML2-WIN (Machine Leaning 2)<br>Open Book, Personal Computer, Internet Access</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>Not allowed:</td>\n",
    "  <td>The use of any form of generative AI (e.g., Copilot, ChatGPT) to assist in solving the exercise is not permitted. <br> However, using such tools as part of the exercise itself (e.g., making API calls to them if required by the task) is allowed. <br> Any form of communication or collaboration with other people is not permitted.</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61847f62",
   "metadata": {},
   "source": [
    "## Evaluation Criteria\n",
    "\n",
    "### <b style=\"color: gray;\">(maximum achievable points: 48)</b>\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Category</th>\n",
    "      <th>Description</th>\n",
    "      <th>Points Distribution</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Code not executable or results not meaningful</td>\n",
    "      <td>The code contains errors that prevent it from running (e.g., syntax errors) or produces results that do not fit the question.</td>\n",
    "      <td>0 points</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Code executable, but with serious deficiencies</td>\n",
    "      <td>The code runs, but the results are incomplete due to major errors (e.g., fundamental errors when reading the data). Only minimal progress is evident.</td>\n",
    "      <td>25% of the maximum achievable points</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Code executable, but with moderate deficiencies</td>\n",
    "      <td>The code runs and delivers partially correct results, but there are significant errors (e.g., the data types of the imported data do not meet the requirements of the question). The results are comprehensible but incomplete or inaccurate.</td>\n",
    "      <td>50% of the maximum achievable points</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Code executable, but with minor deficiencies</td>\n",
    "      <td>The code runs and delivers a largely correct result, but minor errors (e.g., column name misspelled, timestamp not correctly formatted) affect the completeness of the result.</td>\n",
    "      <td>75% of the maximum achievable points</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Code executable and correct</td>\n",
    "      <td>The code runs flawlessly and delivers the correct result without deficiencies.</td>\n",
    "      <td>100% of the maximum achievable points</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8df3dd",
   "metadata": {},
   "source": [
    "## Python Libraries und Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac9011e",
   "metadata": {},
   "source": [
    "## <b>Set Up (This part will <u>not</u> be evaluated!)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e253f40",
   "metadata": {},
   "source": [
    "#### <b>1.) Start a GitHub Codespaces instance based on your fork of this GitHub repository or open the notebook in Colab</b>\n",
    "#### <b>2.) Add API keys to either .env files for Codespaces or to the secrets for Colab</b>\n",
    "#### <b>3.) Please execute the two code cells below as soon as the Codespace/Colab has started and install the libraries</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6e3474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/codespace/.python/current/lib/python3.12/site-packages (25.0.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 25.0.1\n",
      "    Uninstalling pip-25.0.1:\n",
      "      Successfully uninstalled pip-25.0.1\n",
      "Successfully installed pip-25.1.1\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.59 (from langchain-community)\n",
      "  Downloading langchain_core-0.3.60-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain<1.0.0,>=0.3.25 (from langchain-community)\n",
      "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain-community)\n",
      "  Downloading sqlalchemy-2.0.41-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/codespace/.local/lib/python3.12/site-packages (from langchain-community) (6.0.2)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
      "  Downloading aiohttp-3.11.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain-community)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting langsmith<0.4,>=0.1.125 (from langchain-community)\n",
      "  Downloading langsmith-0.3.42-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /home/codespace/.local/lib/python3.12/site-packages (from langchain-community) (2.2.4)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading frozenlist-1.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading multidict-6.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading propcache-0.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading yarl-1.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain<1.0.0,>=0.3.25->langchain-community)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain<1.0.0,>=0.3.25->langchain-community)\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.59->langchain-community)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/codespace/.local/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/codespace/.local/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/codespace/.local/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.125->langchain-community)\n",
      "  Downloading orjson-3.10.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.125->langchain-community)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.125->langchain-community)\n",
      "  Downloading zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
      "Requirement already satisfied: idna in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community)\n",
      "  Downloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain-community)\n",
      "  Downloading greenlet-3.2.2-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
      "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.60-py3-none-any.whl (437 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading langsmith-0.3.42-py3-none-any.whl (360 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading multidict-6.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
      "Downloading orjson-3.10.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (133 kB)\n",
      "Downloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading sqlalchemy-2.0.41-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (349 kB)\n",
      "Downloading zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading frozenlist-1.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (316 kB)\n",
      "Downloading greenlet-3.2.2-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (603 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m603.9/603.9 kB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading propcache-0.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (245 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: zstandard, typing-inspection, tenacity, python-dotenv, pydantic-core, propcache, orjson, mypy-extensions, multidict, marshmallow, jsonpatch, httpx-sse, greenlet, frozenlist, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests-toolbelt, pydantic, aiosignal, pydantic-settings, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30/30\u001b[0m [langchain-community]ngchain-community]ters]\n",
      "\u001b[1A\u001b[2KSuccessfully installed SQLAlchemy-2.0.41 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 annotated-types-0.7.0 dataclasses-json-0.6.7 frozenlist-1.6.0 greenlet-3.2.2 httpx-sse-0.4.0 jsonpatch-1.33 langchain-0.3.25 langchain-community-0.3.24 langchain-core-0.3.60 langchain-text-splitters-0.3.8 langsmith-0.3.42 marshmallow-3.26.1 multidict-6.4.4 mypy-extensions-1.1.0 orjson-3.10.18 propcache-0.3.1 pydantic-2.11.4 pydantic-core-2.33.2 pydantic-settings-2.9.1 python-dotenv-1.1.0 requests-toolbelt-1.0.0 tenacity-9.1.2 typing-inspect-0.9.0 typing-inspection-0.4.0 yarl-1.20.0 zstandard-0.23.0\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /home/codespace/.local/lib/python3.12/site-packages (from faiss-cpu) (2.2.4)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from faiss-cpu) (24.2)\n",
      "Downloading faiss_cpu-1.11.0-cp312-cp312-manylinux_2_28_x86_64.whl (31.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0\n",
      "Collecting groq\n",
      "  Downloading groq-0.25.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from groq) (4.9.0)\n",
      "Collecting distro<2,>=1.7.0 (from groq)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.12/site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from groq) (2.11.4)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.12/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /home/codespace/.local/lib/python3.12/site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.12/site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n",
      "Downloading groq-0.25.0-py3-none-any.whl (129 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: distro, groq\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [groq]\n",
      "\u001b[1A\u001b[2KSuccessfully installed distro-1.9.0 groq-0.25.0\n",
      "Collecting openai\n",
      "  Downloading openai-1.79.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from openai) (2.11.4)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/codespace/.local/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Downloading openai-1.79.0-py3-none-any.whl (683 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m683.3/683.3 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, jiter, openai\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [openai]2m2/3\u001b[0m [openai]\n",
      "\u001b[1A\u001b[2KSuccessfully installed jiter-0.10.0 openai-1.79.0 tqdm-4.67.1\n",
      "Requirement already satisfied: tqdm in /home/codespace/.python/current/lib/python3.12/site-packages (4.67.1)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: tqdm in /home/codespace/.python/current/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (2.6.0+cpu)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (1.15.2)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.31.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (76.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.31.4-py3-none-any.whl (489 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [sentence-transformers]ence-transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed huggingface-hub-0.31.4 regex-2024.11.6 safetensors-0.5.3 sentence-transformers-4.1.0 tokenizers-0.21.1 transformers-4.51.3\n",
      "Requirement already satisfied: huggingface_hub[hf_xet] in /home/codespace/.python/current/lib/python3.12/site-packages (0.31.4)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from huggingface_hub[hf_xet]) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface_hub[hf_xet]) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface_hub[hf_xet]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface_hub[hf_xet]) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface_hub[hf_xet]) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.1 (from huggingface_hub[hf_xet])\n",
      "  Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface_hub[hf_xet]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface_hub[hf_xet]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface_hub[hf_xet]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface_hub[hf_xet]) (2025.1.31)\n",
      "Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hf-xet\n",
      "Successfully installed hf-xet-1.1.2\n",
      "Requirement already satisfied: faiss-cpu in /home/codespace/.python/current/lib/python3.12/site-packages (1.11.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /home/codespace/.local/lib/python3.12/site-packages (from faiss-cpu) (2.2.4)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from faiss-cpu) (24.2)\n",
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.169.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
      "  Downloading google_auth-2.40.1-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-6.31.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: pydantic in /home/codespace/.python/current/lib/python3.12/site-packages (from google-generativeai) (2.11.4)\n",
      "Requirement already satisfied: tqdm in /home/codespace/.python/current/lib/python3.12/site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /home/codespace/.local/lib/python3.12/site-packages (from google-generativeai) (4.12.2)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.25.0rc1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /home/codespace/.local/lib/python3.12/site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/codespace/.local/lib/python3.12/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic->google-generativeai) (0.4.0)\n",
      "Downloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.25.0rc1-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.40.1-py2.py3-none-any.whl (216 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading grpcio-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m118.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading google_api_python_client-2.169.0-py3-none-any.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: uritemplate, pyasn1, protobuf, httplib2, grpcio, cachetools, rsa, pyasn1-modules, proto-plus, googleapis-common-protos, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/17\u001b[0m [google-generativeai]ogle-generativeai]language]\n",
      "\u001b[1A\u001b[2KSuccessfully installed cachetools-5.5.2 google-ai-generativelanguage-0.6.15 google-api-core-2.25.0rc1 google-api-python-client-2.169.0 google-auth-2.40.1 google-auth-httplib2-0.2.0 google-generativeai-0.8.5 googleapis-common-protos-1.70.0 grpcio-1.71.0 grpcio-status-1.71.0 httplib2-0.22.0 proto-plus-1.26.1 protobuf-5.29.4 pyasn1-0.6.1 pyasn1-modules-0.4.2 rsa-4.9.1 uritemplate-4.1.1\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install --upgrade pip\n",
    "!pip install PyPDF2\n",
    "!pip install langchain-community\n",
    "!pip install faiss-cpu\n",
    "!pip install groq\n",
    "!pip install openai\n",
    "!pip install tqdm\n",
    "!pip install sentence-transformers\n",
    "!pip install huggingface_hub[hf_xet]\n",
    "!pip install faiss-cpu\n",
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a875319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import tqdm\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import google.generativeai as genai\n",
    "from groq import Groq\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4b53272",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "groq_key = os.getenv(\"GROQ_API_KEY\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "google_key = os.getenv(\"GOOGLE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c27b72",
   "metadata": {},
   "source": [
    "## <b>Tasks (This part will be evaluated!)</b>\n",
    "### Notes on the following tasks:\n",
    "\n",
    "In this part of the exam, you will build a Retrieval-Augmented Generation (RAG) pipeline that efficiently retrieves medical information from the package inserts of common medications. Imagine you are developing a system for pharmacists or medical professionals to quickly and accurately answer questions about medications. The following five package inserts are provided as your data source:\n",
    "\n",
    "- [data/Amoxicillin.pdf](data/Amoxicillin.pdf)\n",
    "- [data/bisoprolol.pdf](data/bisoprolol.pdf)\n",
    "- [data/citalopram.pdf](data/citalopram.pdf)\n",
    "- [data/metformin.pdf](data/metformin.pdf)\n",
    "- [data/paracetamol.pdf](data/paracetamol.pdf)\n",
    "\n",
    "Your task is to implement a RAG pipeline that retrieves relevant information from these package inserts and integrates it into the answer generation process. Use the provided instructions and your knowledge from the exercises.\n",
    "\n",
    "### Expected Results:\n",
    "\n",
    "1. Read in the provided package inserts and extract all text.\n",
    "2. Split the extracted text into manageable chunks using a text splitter (e.g., `RecursiveCharacterTextSplitter`).\n",
    "3. Create embeddings for the text chunks using a suitable model.\n",
    "4. Index the embeddings in a vector store (e.g., FAISS).\n",
    "5. Develop an appropriate prompt template.\n",
    "6. Build the RAG chain.\n",
    "7. Automatically generate a list of 10 test questions using a language model.\n",
    "8. Let your RAG pipeline answer the 10 generated questions.\n",
    "\n",
    "### Submission documents:\n",
    "\n",
    "Your submission should include:\n",
    "- The completed notebook (this file).\n",
    "- the vector store\n",
    "\n",
    "<b style=\"color:blue;\">Notes on the following tasks:</b>\n",
    "<ul style=\"color:blue;\">\n",
    "  <li>Pay attention to the specific details provided for each task.</li>\n",
    "  <li>Solve each task using Python code. Integrate your code into the code cells for each task.</li>\n",
    "  <li>Present your solution(s) as requested in each task.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6033f466",
   "metadata": {},
   "source": [
    "#### <b>Task (1): Read all 5 PDFs from the 'data' folder and store their content for further use</b>\n",
    "<b>Task details:</b>\n",
    "- The files are located in the 'data' folder..\n",
    "- Display the length of the resulting string (number of characters).\n",
    "- Show the first 100 characters in the notebook output.\n",
    "<b style=\"color: gray;\">(max. points: 2)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdfcb555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  2.30it/s]\n"
     ]
    }
   ],
   "source": [
    "### load the pdf from the path\n",
    "glob_path = \"data/*.pdf\"\n",
    "text = \"\"\n",
    "for pdf_path in tqdm.tqdm(glob.glob(glob_path)):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PdfReader(file)\n",
    "         # Extract text from all pages in the PDF\n",
    "        text += \" \".join(page.extract_text() for page in reader.pages if page.extract_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07233c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in the entire text: \n",
      "Total Numbers: 176575\n",
      "The first 100 characters of the text:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Inhaltsverzeichnis\\nZusammensetzung\\nDarreichungsform und Wirkstoffmenge pro Einheit\\nIndikationen/Anwe'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the number of characters in the text\n",
    "print(f\"Number of characters in the entire text: \")\n",
    "print(f\"Total Numbers: {len(text)}\")\n",
    "\n",
    "# Show the first 100 characters of the text\n",
    "print(f\"The first 100 characters of the text:\")\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31d4887",
   "metadata": {},
   "source": [
    "#### <b>Task (2): Split the text into chunks appropriate for the task. Specify an overlap as well. Give a reason for your choice</b>\n",
    "<b>Task details:</b>\n",
    "- Use the data from the previous task.\n",
    "- Show the total number of chunks in the notebook.\n",
    "- Show the length of the first chunk in the notebook.\n",
    "- Explain you reasoning\n",
    "<b style=\"color: gray;\">(max. points: 4)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93aad8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create a splitter: 2000 characters per chunk with an overlap of 200 characters\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "# Split the extracted text into manageable chunks\n",
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e4a316e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: \n",
      "Total chunks: 98\n",
      "Length of the first chunk: \n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# Show the total number of chunks\n",
    "print(f\"Number of chunks: \")\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "\n",
    "# Show the length of the first chunk\n",
    "print(f\"Length of the first chunk: \")\n",
    "print(len(chunks[0][:200]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23affd5",
   "metadata": {},
   "source": [
    "##### Explanation (double click and add text):\n",
    "This cell creates a RecursiveCharacterTextSplitter with chunk_size=2000 and chunk_overlap=200, then applies it to split the large text into smaller, manageable chunks. The overlap helps preserve context across chunk boundaries, which is important for maintaining semantic meaning when the text is later embedded. The cell also prints the total number of chunks created and a preview of the first chunk (first 200 characters). This chunking step is critical for processing large documents that exceed token limits of embedding models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc3fec8",
   "metadata": {},
   "source": [
    "#### <b>Task (3): Initialize an embedding model</b>\n",
    "<b>Task details:</b>\n",
    "- Choose a suitable embedding model from Huggingface.\n",
    "- [Huggingface models](https://huggingface.co/spaces/mteb/leaderboard).\n",
    "- Consider the size of the model. It should be runnable in your Codespace.\n",
    "- Choose a model appropriate for the data.\n",
    "\n",
    "<b style=\"color: gray;\">(max. points: 2)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed99968e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: ['<s>', '▁Inhalt', 's', 'ver', 'ze', 'ich', 'nis', '▁Zusammen', 'setzung', '▁Dar', 'reich', 'ungs', 'form', '▁und', '▁Wir', 'k', 'stoff', 'men', 'ge', '▁pro', '▁Einheit', '▁Indi', 'ka', 'tionen', '/', 'An', 'wendung', 's', 'möglichkeiten', '▁Dos', 'ierung', '/', 'An', 'wendung', '▁Kontra', 'indik', 'ation', 'en', '▁War', 'n', 'hin', 'weise', '▁und', '▁Vor', 'sicht', 's', 'mas', 's', 'nahme', 'n', '▁Inter', 'aktion', 'en', '▁Schwangerschaft', '/', 'S', 'till', 'zeit', '▁Wirkung', '▁auf', '▁die', '▁Fahrt', 'ü', 'cht', 'igkeit', '▁und', '▁auf', '▁das', '▁Be', 'dien', 'en', '▁von', '▁Maschinen', '▁Un', 'er', 'w', 'ün', 's', 'chte', '▁Wirkung', 'en', '▁Über', 'dos', 'ierung', '▁Eigenschaften', '/', 'Wir', 'k', 'ungen', '▁Pharma', 'ko', 'kin', 'etik', '▁Prä', 'klin', 'ische', '▁Daten', '▁Son', 'stige', '▁Hinweise', '▁Zu', 'lassung', 's', 'nummer', '▁Zu', 'lassung', 'sin', 'haber', 'in', '▁Stand', '▁der', '▁Information', '▁Produkte', '▁Swiss', 'medic', '▁-', 'ge', 'nehm', 'ig', 'te', '▁F', '▁ach', 'information', '▁Am', 'ox', 'ici', 'llin', '</s>']\n",
      "Chunk 1: ['<s>', '▁har', 'm', '▁200', '▁mg', '/4', '▁ml', ',', '▁Pul', 'ver', '▁zur', '▁Herstellung', '▁einer', '▁Sus', 'pension', '▁zum', '▁Ein', 'nehmen', '▁a', 'xa', 'ph', 'arm', '▁ag', '▁Zusammen', 'setzung', '▁Wir', 'k', 'stoffe', '▁Am', 'ox', 'ici', 'llin', 'um', '▁anh', '▁y', 'dri', 'cum', '▁ut', '▁A', 'mo', '▁x', 'ici', 'llin', 'um', '▁tri', 'h', '▁y', 'dri', 'cum', '.', '▁Hilfs', 'stoffe', '▁Aroma', 'tica', ':', '▁V', '▁an', 'illi', 'num', '▁et', '▁alia', ',', '▁Sac', 'char', 'inum', '▁na', 'tric', 'um', ',', '▁ex', '▁ci', 'pien', 's', '▁ad', '▁pul', 'v', '▁er', 'em', '.', '▁Dar', 'reich', 'ungs', 'form', '▁und', '▁Wir', 'k', 'stoff', 'men', 'ge', '▁pro', '▁Einheit', '▁Am', 'ox', 'ici', 'llin', '▁Ax', 'ap', 'har', 'm', '▁200', '▁mg', '/4', '▁ml', '▁Pul', 'v', '▁er', '▁zur', '▁Zu', 'bereit', 'ung', '▁einer', '▁Sus', 'pension', '▁zum', '▁Ein', 'nehmen', ':', '▁200', '▁mg', '/4', '▁ml', '▁zu', 'bereit', 'ete', '▁Sus', 'pension', '▁(1', '</s>']\n",
      "Chunk 2: ['<s>', '▁50', '▁mg', ').', '▁Mit', '▁Erd', 'be', 'er', '▁aroma', '.', '▁Indi', 'ka', 'tionen', '/', 'An', 'wendung', 's', 'möglichkeiten', '▁Am', 'ox', 'ici', 'llin', '▁Ax', 'ap', 'har', 'm', '▁200', '▁mg', '/4', '▁ml', '▁ist', '▁indi', 'ziert', '▁zur', '▁Behandlung', '▁v', '▁on', '▁In', 'fek', 'tionen', ',', '▁die', '▁durch', '▁A', 'mo', '▁x', 'ici', 'llin', '-', 'emp', 'find', 'liche', '▁gr', '▁am', 'nega', 'tiv', '▁e', '▁Erre', 'ger', '▁verursacht', '▁werden', ',', '▁sowie', '▁zur', '▁Behandlung', '▁v', '▁on', '▁Mis', 'ch', 'infektion', 'en', '▁mit', '▁emp', 'find', 'lichen', '▁gr', '▁am', 'posit', 'iv', '▁en', '▁und', '▁gr', '▁am', 'nega', 'tiv', '▁en', '▁Erre', 'ger', 'n', ',', '▁wie', '▁beispielsweise', ':', '▁A', 'tem', 'weg', 's', 'infektion', 'en', '▁Aku', 'te', '▁Ex', 'az', '▁er', 'ba', 'tionen', '▁v', '▁on', '▁chroni', 'scher', '▁Bron', 'chi', 'tis', ',', '▁bakteri', 'elle', '▁P', 'ne', 'umo', 'nie', ',', '▁Bron', 'chie', 'kta', 'sen', '.', '▁O', '</s>']\n",
      "Chunk 3: ['<s>', '▁In', 'fek', 'tionen', '▁Ot', 'itis', '▁media', ',', '▁Sinu', 's', 'itis', ',', '▁T', '▁ons', 'illi', 'tis', ',', '▁Phar', 'y', 'ng', 'itis', '▁(', 'durch', '▁Stre', 'p', 'tok', '▁o', 'kken', '▁verursacht', ').', '▁Har', 'n', 'weg', 's', 'infektion', 'en', '▁Aku', 'te', '▁und', '▁chroni', 'sche', '▁Py', '▁el', 'one', 'ph', 'riti', 's', ',', '▁Z', '▁y', 'stiti', 's', ',', '▁U', 're', 'thri', 'tis', '.', '▁In', 'fek', 'tionen', '▁des', '▁Gastro', 'inte', 'stin', 'al', 'trakt', 'es', '▁Typ', 'hus', '▁und', '▁P', '▁ara', 'typ', 'hus', ',', '▁bakteri', 'elle', '▁Dia', 'rr', 'ho', 'e', '.', '12', '/12/', '20', '24', ',', '▁18:30', '▁com', 'pendi', 'um', '.', 'ch', '▁https', '://', 'com', 'pendi', 'um', '.', 'ch', '/', 'product', '/13', '768', '68', '-', 'amo', 'x', 'ici', 'llin', '-', 'ax', 'ap', 'har', 'm', '-', 'pl', 'v', '-200', '-', 'mg', '-4', 'ml', '-', 'f', '-', '</s>']\n",
      "Chunk 4: ['<s>', '▁/', 'm', 'pro', '▁1', '/7', '▁Bei', '▁Mag', 'en', 'gesch', '▁w', 'ür', '▁oder', '▁Duo', 'den', 'al', 'ul', 'kus', '▁mit', '▁nach', 'ge', 'wiesen', 'er', '▁Heli', 'coba', 'cter', '▁p', '▁y', 'lor', 'i', '-', 'In', 'fek', 'tion', '▁ist', '▁A', 'mo', '▁x', 'ici', 'llin', '▁Ax', 'ap', 'har', 'm', '▁200', '▁mg', '/4', '▁ml', '▁in', '</s>']\n",
      "Chunk 5: ['<s>', '▁Kombination', '▁mit', '▁einem', '▁Proto', 'nen', 'pump', 'en', 'he', 'mmer', '▁(', 'z', '.', 'B', '▁', '.', '▁Om', 'e', 'pr', '▁az', 'ol', ',', '▁Lan', 'so', 'pr', '▁az', 'ol', ')', '▁und', '▁einem', '▁anderen', '▁Anti', 'bio', 'tik', 'um', '▁(', 'z', '.', 'B', '▁', '.', '▁Clari', 'thro', 'm', '▁y', 'cin', '▁oder', '▁Metro', 'ni', 'da', 'z', '▁ol', ')', '▁ang', 'ez', '▁e', 'igt', '.', '▁Ven', 'er', 'ische', '▁Krankheit', 'en', '▁Go', 'nor', 'r', 'ho', 'e', '▁(', 'spezifische', '▁U', 're', 'thri', 'tis', ').', '▁Bei', '▁Ly', 'me', '▁Bor', 're', 'li', 'ose', '▁(', 'Sta', 'dium', '▁I', ',', '▁Er', 'y', 'thema', '▁chronic', 'um', '▁mig', 'r', '▁ans', '▁oder', '▁Er', 'y', 'thema', '▁chronic', 'um', '▁mig', 'r', '▁ans', '▁v', '▁er', 'bunden', '▁mit', '▁f', 'lü', 'chtige', 'n', '▁Ge', 'lenk', '▁er', 'schein', 'ungen', '▁und', '▁f', 'lü', 'chtige', 'n', '▁resp', '▁', '.', '▁begrenzt', 'en', '</s>']\n",
      "Chunk 6: ['<s>', '▁Er', 'schein', 'ungen', ').', '▁Am', 'ox', 'ici', 'llin', '▁Ax', 'ap', 'har', 'm', '▁200', '▁mg', '/4', '▁ml', '▁ist', '▁fer', 'ner', '▁indi', 'ziert', '▁zur', '▁Prop', 'h', '▁', 'yla', 'xe', '▁der', '▁bakteri', 'ellen', '▁En', 'dok', 'ardi', 'tis', '▁bei', '▁za', 'h', 'n', 'medizin', 'ischen', '▁Ein', 'griff', 'en', '▁(', 'z', '.', 'B', '▁', '.', '▁Zahn', 'ex', 'tr', '▁', 'aktion', ',', '▁Z', '▁ah', 'n', 'stein', 'ent', 'fer', 'nung', ',', '▁Z', '▁ah', 'n', 'fül', 'lung', '),', '▁En', 'dos', 'k', '▁op', 'ien', '▁und', '▁anderen', '▁Oper', '▁a', 'tionen', ',', '▁die', '▁häufig', '▁v', '▁on', '▁einer', '▁Bakteri', 'ä', 'mie', '▁begleitet', '▁sind', '▁und', '▁die', '▁das', '▁Ris', 'ik', '▁o', '▁einer', '▁En', 'dok', 'ardi', 'tis', '▁bei', '▁gewisse', 'n', '▁P', '▁er', 'son', 'en', '▁mit', '▁Herz', 'schä', 'den', '▁erhöhen', '.', '▁Eine', '▁Ein', 'z', '▁el', 'dos', 'is', '▁v', '▁on', '▁3', '▁g', '▁A', 'mo', '</s>']\n",
      "Chunk 7: ['<s>', '▁', 'llin', '▁Ax', 'ap', 'har', 'm', '▁kann', '▁v', '▁er', 'wende', 't', '▁werden', ':', '▁zur', '▁Behandlung', '▁der', '▁Go', 'nor', 'r', 'ho', 'e', '▁(', 'spezifische', '▁U', 're', 'thri', 'tis', ')', '▁und', '▁un', 'k', '▁om', 'pli', 'ziert', 'er', '▁In', 'fek', 'tionen', '▁der', '▁unter', 'en', '▁Har', 'n', '▁weg', 'e', '▁(', 'Z', '▁y', 'stiti', 's', ',', '▁bakteri', 'elle', '▁U', 're', 'thri', 'tis', ');', '▁zur', '▁Prop', 'h', '▁', 'yla', 'xe', '▁der', '▁bakteri', 'ellen', '▁En', 'dok', 'ardi', 'tis', '▁bei', '▁za', 'h', 'n', 'medizin', 'ischen', '▁Ein', 'griff', 'en', '▁(', 'z', '.', 'B', '▁', '.', '▁Zahn', 'ex', 'tr', '▁', 'aktion', ',', '▁Z', '▁ah', 'n', 'stein', 'ent', 'fer', 'nung', ',', '▁Z', '▁ah', 'n', 'fül', 'lung', '),', '▁En', 'dos', 'k', '▁op', 'ien', '▁und', '▁anderen', '▁Oper', '▁a', 'tionen', ',', '▁die', '▁häufig', '▁v', '▁on', '▁einer', '▁Bakteri', 'ä', 'mie', '▁begleitet', '</s>']\n",
      "Chunk 8: ['<s>', '▁das', '▁Ris', 'ik', '▁o', '▁einer', '▁En', 'dok', 'ardi', 'tis', '▁bei', '▁gewisse', 'n', '▁Personen', '▁mit', '▁Herz', 'schä', 'den', '▁erhöhen', '.', '▁Off', 'izi', 'elle', '▁Empfehlung', 'en', '▁zum', '▁ange', 'messen', 'en', '▁Geb', 'r', '▁auch', '▁v', '▁on', '▁Anti', 'bio', 'tika', '▁sollen', '▁beach', 'tet', '▁werden', ',', '▁insbesondere', '▁An', '▁', 'wendung', 'se', 'mpf', 'e', 'hl', 'ungen', '▁zur', '▁Ver', 'hinder', 'ung', '▁der', '▁Zu', 'nahme', '▁der', '▁Anti', 'bio', 'tika', 'res', 'isten', 'z', '.', '▁Dos', 'ierung', '/', 'An', 'wendung', '▁Die', '▁Dos', 'ierung', '▁ist', '▁', 'abhängig', '▁v', '▁on', '▁der', '▁An', '▁', 'wendung', 's', 'art', ',', '▁v', '▁om', '▁Alter', '▁', ',', '▁vom', '▁Gewicht', '▁und', '▁v', '▁on', '▁der', '▁N', 'ieren', 'funktion', '▁des', '▁P', '▁ati', 'enten', '▁sowie', '▁v', '▁om', '▁Schwer', 'e', 'gr', '▁ad', '▁der', '▁In', 'fek', 'tion', '▁und', '▁der', '▁Emp', 'fi', 'ndlich', 'k', '▁e', 'it', '▁des', '▁K', '▁ei', 'mes', '</s>']\n",
      "Chunk 9: ['<s>', '▁The', 'r', '▁apie', '▁sollte', '▁über', '▁48', '▁bis', '▁72', '▁Stunden', '▁nach', '▁Er', 'reichen', '▁einer', '▁klin', 'ischen', '▁Wirkung', '▁fort', 'gesetzt', '▁werden', '.', '▁Bei', '▁einer', '▁In', 'fek', 'tion', ',', '▁die', '▁durch', '▁β', '-', '</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6300/3884881723.py:21: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # For generating embeddings for text chunks\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=128, model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "token_split_texts = []\n",
    "for text in chunks:\n",
    "    token_split_texts += token_splitter.split_text(text)\n",
    "\n",
    "model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "tokenized_chunks = []\n",
    "for i, text in enumerate(token_split_texts[:10]):\n",
    "    # Tokenize each chunk\n",
    "    encoded_input = model.tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = model.tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0].tolist())\n",
    "    tokenized_chunks.append(tokens)\n",
    "    print(f\"Chunk {i}: {tokens}\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "chunk_embeddings = model.encode(token_split_texts, convert_to_numpy=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b80dc61",
   "metadata": {},
   "source": [
    "#### <b>Task (4): Create a vector store</b>\n",
    "<b>Task details:</b>\n",
    "- Create a vector store\n",
    "- store the vector store (this is also helpful in case the codespace or colab needs a restart)\n",
    "<b style=\"color: gray;\">(max. achievable points: 6)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "077a187d-05be-4c30-a367-a4e1a19d4466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "d = chunk_embeddings.shape[1]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d178ea2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings in FAISS index: 483\n"
     ]
    }
   ],
   "source": [
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(chunk_embeddings)\n",
    "print(\"Number of embeddings in FAISS index:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17c8aa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(token_split_texts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cfa1d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483\n"
     ]
    }
   ],
   "source": [
    "index = faiss.read_index(\"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"rb\") as f:\n",
    "    token_split_texts = pickle.load(f)\n",
    "print(len(token_split_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e369261",
   "metadata": {},
   "source": [
    "#### <b>Task (5): Create a retriever function.</b>\n",
    "<b>Task details:</b>\n",
    "- Create a retriever function\n",
    "- Define the number of documents the retriever should return.\n",
    "- Test the retriever with the following query: `\"Welche Dosierung von Amoxicillin Axapharm wird für die Behandlung einer Endokarditis-Prophylaxe bei Erwachsenen empfohlen?\"`\n",
    "- If the retrieved chunks are not relevant, increase the number of chunks to be retrieved and repeat the query. \n",
    "- It does not have to be perfect; if nothing improves, continue with the current result.\n",
    "<b style=\"color: gray;\">(max. achievable points: 6)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "034ae6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_texts(query, k, index, chunks, model):\n",
    "    \"\"\"\n",
    "    Retrieve the top k similar text chunks and their embeddings for a given query.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    retrieved_texts = [token_split_texts[i] for i in indices[0]]\n",
    "    return retrieved_texts, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8465da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Welche Dosierung +von Amoxicillin Axapharm wird für die Behandlung einer Endokarditis-Prophylaxe bei Erwachsenen empfohlen?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef402de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Erscheinungen). Amoxicillin Axapharm 200 mg/4 ml ist ferner indiziert zur Proph ylaxe der bakteriellen Endokarditis bei zahnmedizinischen Eingriffen (z.B . Zahnextr aktion, Z ahnsteinentfernung, Z ahnfüllung), Endosk opien und anderen Oper ationen, die häufig v on einer Bakteriämie begleitet sind und die das Risik o einer Endokarditis bei gewissen P ersonen mit Herzschäden erhöhen. Eine Einz eldosis v on 3 g Amo xici', '-Clear ance v on <10 ml/min. Zusätzlich erhalten Erw achsene 1 g parenter al oder 750 mg or al und Kinder 15 mg/kg parenter al nach jeder Dialyse. Art der Anwendung Amoxicillin Axapharm 200 mg/4 ml kann ohne Wirkungsv erlust zu den Mahlz eiten eingenommen werden. Die Suspension ist v or jedem Gebr auch zu schütteln. Bei der Behandlung v on Kleinkindern ist zu beachten, dass die zubereitete Suspension maximal 10 T age haltbar ist. Kontraindikationen Überempfindlichk', 'Harnabfluss im Katheter regelmässig k ontrolliert werden. Patienten mit Nierenfunktionsstörungen Bei Niereninsuffizienz ist die A usscheidung v on Amo xicillin v erzögert. Amo xicillin Axapharm 200 mg/4 ml kann anhand der Kreatinin- Clearance (KrCl) wie folgt dosiert werden:12/12/2024, 18:30 compendium.ch https://compendium.ch/product/1376868-amoxicillin-axapharm-plv-200-mg-4ml-f-susp/'], array([[5.377816 , 6.3442717, 6.9978004]], dtype=float32))\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Testen des retrievers\n",
    "retrieved_texts = retrieve_texts(query, 3, index, chunks, model)\n",
    "\n",
    "print(retrieved_texts)\n",
    "print(len(retrieved_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cd43ac",
   "metadata": {},
   "source": [
    "#### <b>Task (6): Implement a reusable RAG function and prompt template</b>\n",
    "<b>Task details:</b>\n",
    "- Write a function `get_answer_and_documents` that answers a question using your RAG pipeline.\n",
    "- The function should:\n",
    "  - Take as parameters: the question (`question`), the number of documents to retrieve (`k`), the FAISS index (`index`), and the list of text chunks (`chunks`).\n",
    "  - The prompt template should be tailored to the medical context, address medical professionals, and instruct the model to answer concisely and in German, using only the provided context. This is part of the task.\n",
    "  - Return both the answer and the retrieved documents.\n",
    "- Test the function with the question: `Ab welcher Kreatinin-Clearance ist die Einnahme von Metformin kontraindiziert?`\n",
    "\n",
    "<b style=\"color: gray;\">(max. achievable points: 8)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae8ae141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set language model and output parser\n",
    "def answer_query(query, k, index,texts):\n",
    "\n",
    "    \"\"\"\n",
    "    Retrieve the top k similar text chunks for the given query using the retriever,\n",
    "    inject them into a prompt, and send it to the Groq LLM to obtain an answer.\n",
    "    \n",
    "    Parameters:\n",
    "    - query (str): The user's query.\n",
    "    - k (int): Number of retrieved documents to use.\n",
    "    - groq_api_key (str): Your Groq API key.\n",
    "    \n",
    "    Returns:\n",
    "    - answer (str): The answer generated by the LLM.\n",
    "    \"\"\"\n",
    "    # Retrieve the top k documents using your retriever function.\n",
    "    # This retriever uses the following definition:\n",
    "    # def retrieve(query, k):\n",
    "    #     query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    #     distances, indices = index.search(query_embedding, k)\n",
    "    #     retrieved_texts = [token_split_texts[i] for i in indices[0]]\n",
    "    #     retrieved_embeddings = np.array([chunk_embeddings[i] for i in indices[0]])\n",
    "    #     return retrieved_texts, retrieved_embeddings, distances\n",
    "    model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    retrieved_texts, _ = retrieve_texts(query, k, index, texts, model)\n",
    "    \n",
    "    # Combine the retrieved documents into a single context block.\n",
    "    context = \"\\n\\n\".join(retrieved_texts)\n",
    "    \n",
    "    # Build a prompt that instructs the LLM to answer the query based on the context.\n",
    "    prompt = (\n",
    "        \"Answer the following question using the provided context. \"\n",
    "        \"Explain it as if you are explaining it to a 5 year old.\\n\\n\"\n",
    "        \"Context:\\n\" + context + \"\\n\\n\"\n",
    "        \"Question: \" + query + \"\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    \n",
    "    # Initialize the Groq client and send the prompt.\n",
    "    client = Groq(api_key=groq_key)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    llm = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"llama-3.3-70b-versatile\"\n",
    "    )\n",
    "    \n",
    "    # Extract and return the answer.\n",
    "    answer = llm.choices[0].message.content\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c380d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query\n",
    "query = \"Ab welcher Kreatinin-Clearance ist die Einnahme von Metformin kontraindiziert?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa1a3d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo kleiner Freund! \n",
      "\n",
      "Stell dir vor, dein Körper hat eine spezielle Filteranlage, die called Nieren. Sie helfen, den Körper sauber zu halten. Die Kreatinin-Clearance ist ein Wert, der zeigt, wie gut diese Filteranlage funktioniert.\n",
      "\n",
      "Metformin ist ein Medikament, das manchmal zusammen mit anderen Medikamenten eingenommen wird. Es ist wichtig, dass der Arzt die richtige Dosis verschreibt, besonders wenn die Nieren nicht so gut funktionieren.\n",
      "\n",
      "Wenn die Kreatinin-Clearance unter 30 ml/min fällt, bedeutet das, dass die Nieren nicht so gut funktionieren. In diesem Fall ist es nicht gut, Metformin zu nehmen. Es ist wie ein Warnschild, das sagt: \"Vorsicht, hier ist es nicht sicher!\"\n",
      "\n",
      "Also, ab einer Kreatinin-Clearance von weniger als 30 ml/min ist die Einnahme von Metformin kontraindiziert. Das bedeutet, dass der Arzt sagen wird: \"Nein, du darfst kein Metformin nehmen, weil deine Nieren nicht gut genug funktionieren.\" \n",
      "\n",
      "Ich hoffe, das hilft dir, es zu verstehen!\n"
     ]
    }
   ],
   "source": [
    "# print result of test query with your chain (hint: input is a dictionary)\n",
    "print(answer_query(query, 4, index, chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34584dc",
   "metadata": {},
   "source": [
    "#### <b>Task (7): Implement a HyDE Query Transformation for RAG</b>\n",
    "<b>Task details:</b>\n",
    "- Implement a function that applies the HyDE strategy in your RAG pipeline.\n",
    "- add your HyDe transformation to your pipeline\n",
    "- Display the intermediate transformation (print statement within function is enough) and the final answer in the notebook.\n",
    "<b style=\"color: gray;\">(max. achievable points: 6)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2746cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def rewrite_query_hyde(query):\n",
    "    \n",
    "def rewrite_query(query, groq_api_key):\n",
    "    \"\"\"\n",
    "    Rewrite the user's query into to potentially improve retrieval.\n",
    "    Parameters:\n",
    "    - query (str): The original user query.\n",
    "    - groq_api_key (str): Your Groq API key.\n",
    "    \n",
    "    Returns:\n",
    "    - rewritten_query (str): The rewritten query.\n",
    "    \"\"\"\n",
    "    client = Groq(api_key=groq_key)\n",
    "    # Build a prompt for rewriting the query\n",
    "    rewriting_prompt = (\n",
    "        \"Rewrite the following query into a format, such that it can be answered by looking at medical guidelines. \"\n",
    "        \"Keep the keywords but ensure that it is close to a format, such as in medical guidelines. Just answer with the rewritten query\\n\\n\"\n",
    "        \"Query: \" + query\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": rewriting_prompt}\n",
    "    ]\n",
    "    \n",
    "    # Use the same model (for example, llama) to perform query rewriting\n",
    "    llm = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "    )\n",
    "    rewritten_query = llm.choices[0].message.content.strip()\n",
    "    return rewritten_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79bfb9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def answer_query_with_rewriting(query, k, index, texts):\n",
    "    \n",
    "def answer_query_with_rewriting(query, k, index, texts, groq_api_key):\n",
    "    \"\"\"\n",
    "    Retrieve the top k similar text chunks for the given query using a retrieval method\n",
    "    with query rewriting, inject them into a prompt, and send it to the Groq LLM (using llama)\n",
    "    to obtain an answer.\n",
    "    \n",
    "    Parameters:\n",
    "    - query (str): The user's query.\n",
    "    - k (int): Number of retrieved documents to use.\n",
    "    - index: The FAISS index.\n",
    "    - texts (list): The tokenized text chunks mapping.\n",
    "    - groq_api_key (str): Your Groq API key.\n",
    "    \n",
    "    Returns:\n",
    "    - answer (str): The answer generated by the LLM.\n",
    "    \"\"\"\n",
    "    model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    # Use the new retrieval function with query rewriting.\n",
    "    rewritten_query = rewrite_query(query, groq_key)    \n",
    "    print(\"Rewritten Query:\", rewritten_query) ## FYI\n",
    "\n",
    "    retrieved_texts, _ = retrieve_texts(rewritten_query, k, index, texts, model)\n",
    "    \n",
    "    # Combine the retrieved documents into a single context block.\n",
    "    context = \"\\n\\n\".join(retrieved_texts)\n",
    "    \n",
    "    # Build a prompt that instructs the LLM to answer the query based on the context.\n",
    "    prompt = (\n",
    "        \"Answer the following question using the provided context. \"\n",
    "        \"Explain it as if you are explaining it to a 5 year old.\\n\\n\"\n",
    "        \"Context:\\n\" + context + \"\\n\\n\"\n",
    "        \"Question: \" + query + \"\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    \n",
    "    # Initialize the Groq client and send the prompt.\n",
    "    client = Groq(api_key=groq_api_key)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    llm = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"llama-3.3-70b-versatile\"\n",
    "    )\n",
    "    \n",
    "    # Extract and return the answer.\n",
    "    answer = llm.choices[0].message.content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b1a8598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewritten Query: Was sind die diagnostischen Kriterien für Asthma und welcher Faktor wird in den aktuellen medizinischen Leitlinien als wichtigster Faktor bei der Diagnosestellung von Asthma angesehen?\n",
      "LLM Answer: Das ist eine gute Frage!\n",
      "\n",
      "Also, wenn wir über Asthma sprechen, ist es wichtig zu wissen, dass es viele verschiedene Dinge sind, die helfen können, Asthma zu diagnostizieren. Aber der wichtigste Faktor ist... (dramatische Pause) ...die Symptome des Patienten!\n",
      "\n",
      "Wenn jemand Atemnot, Husten, Keuchen oder andere Probleme mit dem Atmen hat, kann das ein Zeichen für Asthma sein. Die Ärzte werden dann verschiedene Tests machen, wie zum Beispiel eine Lungenfunktionstest, um zu sehen, wie gut die Lungen funktionieren. Aber die Symptome sind der wichtigste Faktor, um zu entscheiden, ob jemand Asthma hat oder nicht.\n",
      "\n",
      "Es ist wie wenn du ein Puzzle löst. Die Symptome sind die Teile, die du zusammenfügen musst, um das Bild von Asthma zu sehen. Und wenn du das Bild siehst, kannst du dann die richtige Behandlung finden, um das Asthma zu kontrollieren.\n",
      "\n",
      "Ich hoffe, das hilft!\n"
     ]
    }
   ],
   "source": [
    "query = \"Was ist der wichtigste Faktor bei der Diagnostizierung von Asthma?\"\n",
    "answer = answer_query_with_rewriting(query, 5, index, chunks, groq_key)\n",
    "print(\"LLM Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bd20f4",
   "metadata": {},
   "source": [
    "#### <b>Task (7): Generate a list of test questions</b>\n",
    "<b>Task details:</b>\n",
    "- Create a Python list with 10 questions about the provided medications.\n",
    "- The questions should be automatically generated using a language model.\n",
    "- You may use chunks from the package inserts as inspiration, but this is not required.\n",
    "- At the end, print out your list of questions.\n",
    "\n",
    "<b style=\"color: gray;\">(max. achievable points: 6)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "504ca399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import httpx  # Ensure you're catching the correct timeout exception\n",
    "import random\n",
    "from openai import OpenAI\n",
    "def generate_questions_for_random_chunks(chunks, num_chunks=10, max_retries=3):\n",
    "    \"\"\"\n",
    "    Randomly selects a specified number of text chunks from the provided list,\n",
    "    then generates a question for each selected chunk using the Groq LLM.\n",
    "\n",
    "    Parameters:\n",
    "    - chunks (list): List of text chunks.\n",
    "    - groq_api_key (str): Your Groq API key.\n",
    "    - num_chunks (int): Number of chunks to select randomly (default is 10).\n",
    "\n",
    "    Returns:\n",
    "    - questions (list of tuples): Each tuple contains (chunk, generated_question).\n",
    "    \"\"\"\n",
    "    # Randomly select the desired number of chunks.\n",
    "    selected_chunks = random.sample(chunks, num_chunks)\n",
    "    \n",
    "    # Initialize the Groq client once\n",
    "    client = OpenAI(api_key=openai.api_key)\n",
    "    \n",
    "    questions = []\n",
    "    for chunk in tqdm.tqdm(selected_chunks):\n",
    "        # Build a prompt that asks the LLM to generate a question based on the chunk.\n",
    "        prompt = (\n",
    "            \"Based on the following text, generate an insightful question that covers its key content:\\n\\n\"\n",
    "            \"Text:\\n\" + chunk + \"\\n\\n\"\n",
    "            \"Question:\"\n",
    "        )\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        generated_question = None\n",
    "        attempt = 0\n",
    "        \n",
    "        # Try calling the API with simple retry logic.\n",
    "        while attempt < max_retries:\n",
    "            try:\n",
    "                llm_response = client.chat.completions.create(\n",
    "                     model=\"gpt-4o-mini\",\n",
    "                    messages=messages\n",
    "                )\n",
    "                generated_question = llm_response.choices[0].message.content.strip()\n",
    "                break  # Exit the loop if successful.\n",
    "            except httpx.ReadTimeout:\n",
    "                attempt += 1\n",
    "                print(f\"Timeout occurred for chunk. Retrying attempt {attempt}/{max_retries}...\")\n",
    "                time.sleep(2)  # Wait a bit before retrying.\n",
    "        \n",
    "        # If all attempts fail, use an error message as the generated question.\n",
    "        if generated_question is None:\n",
    "            generated_question = \"Error: Failed to generate question after several retries.\"\n",
    "        \n",
    "        questions.append((chunk, generated_question))\n",
    "    \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43066bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:09<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "Besch werden muss die Behandlung mit Metformin sofort v orübergehend unterbrochen werden.\n",
      "Iodhaltige...\n",
      "Generated Question: What precautions should be taken regarding Metformin treatment in patients undergoing X-ray examinations with iodinated contrast media, and what are the implications for renal and cardiac function?\n",
      "\n",
      "Chunk 2:\n",
      "26-30 kg 8-10 Jahre 1500-2000 mg 4× 400 mg\n",
      "31-40 kg 10-12 Jahre 2000 mg 4× 400 mg\n",
      "Schwere Infektione...\n",
      "Generated Question: What are the specific dosage guidelines for administering Amoxicillin based on patient weight, age, and renal function as described in the provided text?\n",
      "\n",
      "Chunk 3:\n",
      "Hereditäre k onstitutionelle Hyperbilirubinämie (Morbus Meulengr acht).\n",
      "Warnhinweise u nd Vorsichtsm...\n",
      "Generated Question: What are the key precautions and potential risks associated with the use of Paracetamol in patients with hereditary constitutional hyperbilirubinemia and other underlying health conditions?\n",
      "\n",
      "Chunk 4:\n",
      "Zusammensetzung\n",
      "Darreichungsform und Wirkstoffmenge pro Einheit\n",
      "Indikationen/Anwendungsmöglichkeiten...\n",
      "Generated Question: What are the key therapeutic uses, dosage guidelines, and potential side effects of Citalopram, as detailed in the product information provided?\n",
      "\n",
      "Chunk 5:\n",
      "Beibehaltung der Metformin-Dosis kann bei nicht adäquater Blutzuck erkontrolle eine abendliche Insul...\n",
      "Generated Question: What are the recommended adjustments for Metformin dosage in patients with Type 1 Diabetes and varying levels of kidney function, particularly in relation to the risk of lactic acidosis?\n",
      "\n",
      "Chunk 6:\n",
      "(verminderte Nahrungsaufnahme und Körpergewichtzunahme) als auch beim Embry o/Fötus (erhöhte Z ahl v...\n",
      "Generated Question: What are the potential effects of Bisoprolol-HCT treatment on patients and diagnostic methods, particularly regarding fetal development and laboratory parameters?\n",
      "\n",
      "Chunk 7:\n",
      "Seite ist mit der Prägung «10» v ersehen. Die Lactab kann in gleiche Hälften geteilt werden.\n",
      "Indikat...\n",
      "Generated Question: What are the key considerations and recommendations for the use of Bisoprolol-HCT-Mepha in patients with specific health conditions, such as renal impairments and age-related factors?\n",
      "\n",
      "Chunk 8:\n",
      "gener alisierte exanthematöse Pustulosis (AGEP) wurden bei P atienten unter Behandlung mit Beta-Lakt...\n",
      "Generated Question: What are the key precautions and potential complications associated with the use of Amoxicillin, especially in patients with a history of drug reactions or those taking oral anticoagulants?\n",
      "\n",
      "Chunk 9:\n",
      "Herz-Minuten- Volumens und damit eine V erminderung des Blutdrucks. Der Blutdruckabfall wird durch S...\n",
      "Generated Question: What are the key mechanisms by which thiazide diuretics lower blood pressure, and how do these effects change over time during treatment?\n",
      "\n",
      "Chunk 10:\n",
      "hoher Dosen v on Paracetamol (≥500 mg/kg) an männliche R atten resultierte in v erminderter F ertili...\n",
      "Generated Question: What are the potential side effects of high doses of Paracetamol on male fertility, and what storage guidelines should be followed for Paracetamol-containing medications?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "questions = generate_questions_for_random_chunks(chunks, num_chunks=10, max_retries=2)\n",
    "for idx, (chunk, question) in enumerate(questions, start=1):\n",
    "    print(f\"Chunk {idx}:\\n{chunk[:100]}...\\nGenerated Question: {question}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76e6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i, question in enumerate(questions):\n",
    "    #i +=1\n",
    "    #print(\"Frage \" + str(1) + \": \" + question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67e3225",
   "metadata": {},
   "source": [
    "#### <b>Task (8): Let your retriever answer the 10 generated questions.</b>\n",
    "<b>Task details:</b>\n",
    "- Use the 10 generated questions and have them answered by your RAG chain.\n",
    "- For each question, output both the retrieved documents and the answer.\n",
    "- Provide your own assessment of whether your chain works well or not.\n",
    "- Give an example of what worked well and what did not.\n",
    "\n",
    "<b style=\"color: gray;\">(max. achievable points: 6)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "909478da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewritten Query: Diagnostische Kriterien für Asthma: Welche Faktoren haben bei der Diagnostizierung von Asthma die größte Priorität?\n",
      "Hallo kleiner Freund! Asthma ist eine Erkrankung, bei der es schwer ist, Luft in die Lungen zu bekommen. Wenn du Asthma hast, kann es passieren, dass du plötzlich schwer atmen musst oder dass du husten musst.\n",
      "\n",
      "Der wichtigste Faktor bei der Diagnostizierung von Asthma ist, wie oft und wie schwer du Atemnot hast. Wenn du oft Atemnot hast, besonders nachdem du viel gelaufen bist oder wenn du krank bist, muss dein Arzt das überprüfen.\n",
      "\n",
      "Außerdem gibt es einige Medikamente, die nicht gut für Menschen mit Asthma sind. Wenn du ein Medikament nimmst, das dich atmen lässt, aber du immer noch Atemnot hast, muss dein Arzt das überprüfen.\n",
      "\n",
      "Es gibt auch einige Dinge, die Asthma schlimmer machen können, wie zum Beispiel Rauch, Staub oder bestimmte Gerüche. Wenn du weißt, was dich atmen lässt, kannst du das deinem Arzt sagen, damit er dir helfen kann.\n",
      "\n",
      "Der Arzt wird auch nach anderen Dingen fragen, wie zum Beispiel, ob du jemals einen Anfall von Atemnot hattest oder ob du andere Erkrankungen hast. Alles zusammen hilft dem Arzt, zu bestimmen, ob du Asthma hast und wie wichtig es ist, dass du behandelt wirst.\n",
      "\n",
      "Also, der wichtigste Faktor bei der Diagnostizierung von Asthma ist, wie oft und wie schwer du Atemnot hast und wie du auf verschiedene Dinge reagierst. Dein Arzt wird dir helfen, das alles zu überprüfen und eine gute Behandlung für dich zu finden!\n",
      "Rewritten Query: Bei der Diagnostizierung von Asthma: Welche Kriterien oder Symptome sollten als wichtigster Faktor zur Bestätigung der Erkrankung berücksichtigt werden?\n",
      "Kleine Fragen! \n",
      "\n",
      "Der wichtigste Faktor bei der Diagnostizierung von Asthma ist... (dramatische Pause) ...die Symptome, die du hast! \n",
      "\n",
      "Du siehst, Asthma ist eine Krankheit, die die Atemwege betrifft. Wenn du Asthma hast, kannst du Symptome wie Atemnot, Husten, Keuchen oder Enge in der Brust haben. Dein Arzt wird dich fragen, ob du diese Symptome hast und wie oft sie auftreten. Er wird auch andere Dinge überprüfen, wie zum Beispiel, ob du Allergien hast oder ob du in der Vergangenheit ähnliche Symptome hattest.\n",
      "\n",
      "Es gibt auch einige Tests, die dein Arzt machen kann, um zu sehen, ob du Asthma hast. Ein Beispiel ist ein Test, bei dem du durch ein Rohr atmen musst, um zu sehen, wie gut deine Lungen funktionieren. Aber die Symptome, die du hast, sind der wichtigste Faktor bei der Diagnostizierung von Asthma.\n",
      "\n",
      "Also, wenn du denkst, dass du Asthma hast, solltest du mit deinem Arzt sprechen und ihm alle deine Symptome erzählen. Er kann dann helfen, herauszufinden, was los ist und wie du behandelt werden kannst.\n",
      "Rewritten Query: Kriterien für die Diagnostizierung von Asthma: Welche Faktoren sind für die Feststellung einer Asthma-Erkrankung am wichtigsten zu berücksichtigen?\n",
      "Hallo kleiner Freund! \n",
      "\n",
      "Asthma ist eine Erkrankung, die die Luftwege in deiner Lunge betrifft. Wenn du Asthma hast, kann es passieren, dass deine Luftwege eng werden und du Schwierigkeiten hast, Luft zu holen.\n",
      "\n",
      "Der wichtigste Faktor bei der Diagnostizierung von Asthma ist... **das Husten und Keuchen**! Wenn du oft hustest oder keuchst, besonders nachts oder nach körperlicher Anstrengung, kann das ein Zeichen für Asthma sein.\n",
      "\n",
      "Aber es gibt noch andere Dinge, die der Arzt überprüfen muss, um sicherzustellen, dass du Asthma hast. Er wird dich Fragen stellen, wie oft du hustest oder keuchst, und ob du jemals Schwierigkeiten hattest, Luft zu holen. Er wird dich auch körperlich untersuchen und eventuell einige Tests machen, um sicherzustellen, dass du Asthma hast.\n",
      "\n",
      "Es ist wichtig, dass du mit deinem Arzt sprichst, wenn du denkst, dass du Asthma hast. Er kann dir helfen, die richtige Diagnose zu stellen und die beste Behandlung für dich zu finden.\n",
      "Rewritten Query: Welche klinischen Kriterien oder Symptome sind für die Diagnostizierung von Asthma gemäß den aktuellen medizinischen Leitlinien am wichtigsten?\n",
      "Also, das ist eine gute Frage!\n",
      "\n",
      "Stell dir vor, du hast ein bisschen Schwierigkeiten beim Atmen. Das ist nicht schön, oder? Wenn jemand Asthma hat, bedeutet das, dass seine Atemwege ein bisschen \"verstopft\" sind, wie ein Rohr, das zu eng ist.\n",
      "\n",
      "Der wichtigste Faktor bei der Diagnostizierung von Asthma ist... (dramatische Pause) ...die Art und Weise, wie du atmest! Wenn du beim Atmen Schwierigkeiten hast, wie zum Beispiel, wenn du viel hustest oder wenn du schnell atmest, kann das ein Zeichen für Asthma sein.\n",
      "\n",
      "Außerdem gibt es noch andere Dinge, die der Arzt überprüfen kann, wie zum Beispiel, ob du eine Geschichte von Atemwegserkrankungen hast oder ob du bestimmte Symptome wie Husten oder Keuchen hast.\n",
      "\n",
      "Aber das Wichtigste ist, dass der Arzt deine Atemwege untersucht und feststellt, ob sie \"verstopft\" sind oder nicht. Das ist wie ein Detektiv, der herausfinden möchte, was los ist!\n",
      "Rewritten Query: Bei der Diagnostizierung von Asthma: Welche Symptome oder Kriterien haben die größte Aussagekraft für die Feststellung einer Asthma-Erkrankung?\n",
      "Hallo Kleiner!\n",
      "\n",
      "Also, wenn wir über Asthma sprechen, ist es wichtig, dass wir wissen, was los ist. Der wichtigste Faktor bei der Diagnostizierung von Asthma ist, wie du atmest! Wenn du Schwierigkeiten hast, Luft zu holen, oder wenn du hustest oder keuchst, dann ist das ein Zeichen, dass etwas nicht stimmt.\n",
      "\n",
      "Die Ärzte werden dann nachschauen, ob du bestimmte Symptome hast, wie zum Beispiel:\n",
      "\n",
      "* Schwierigkeiten beim Atmen\n",
      "* Husten\n",
      "* Keuchen\n",
      "* Brustschmerzen\n",
      "\n",
      "Sie werden auch nachschauen, ob du bestimmte Auslöser hast, wie zum Beispiel:\n",
      "\n",
      "* Allergien\n",
      "* Infektionen\n",
      "* Überbelastung\n",
      "\n",
      "Und sie werden auch nachschauen, wie deine Lungen funktionieren, indem sie bestimmte Tests machen.\n",
      "\n",
      "Aber der wichtigste Faktor ist, dass sie mit dir sprechen und wissen, wie du dich fühlst. Sie werden dann zusammen mit dir herausfinden, ob du Asthma hast und wie sie es behandeln können.\n",
      "\n",
      "Also, wenn du Schwierigkeiten beim Atmen hast, musst du zum Arzt gehen und mit ihm sprechen!\n",
      "Rewritten Query: Klinische Kriterien für die Diagnostizierung von Asthma: Welche Faktoren haben den größten Stellenwert bei der Feststellung einer Asthma-Erkrankung?\n",
      "Hallo kleiner Freund! \n",
      "\n",
      "Asthma ist eine Krankheit, die die Atemwege im Körper betrifft. Wenn jemand Asthma hat, kann es schwer werden, Luft in die Lungen zu bekommen. Der wichtigste Faktor bei der Diagnostizierung von Asthma ist, wie jemand atmet und wie seine Atemwege reagieren.\n",
      "\n",
      "Stell dir vor, du hast einen kleinen Schlauch, durch den du Luft in deine Lungen blasen musst. Wenn der Schlauch eng wird, kann nicht so viel Luft hindurchkommen. Das ist ähnlich, was bei Asthma passiert. Die Atemwege werden enger und es wird schwerer, Luft zu bekommen.\n",
      "\n",
      "Wenn jemand Asthma hat, kann er oder sie folgende Symptome haben:\n",
      "\n",
      "* Schwierigkeiten beim Atmen\n",
      "* Keuchen oder Rasseln in der Brust\n",
      "* Husten\n",
      "* Enge in der Brust\n",
      "\n",
      "Um Asthma zu diagnostizieren, muss ein Arzt oder eine Ärztin verschiedene Tests durchführen, wie zum Beispiel:\n",
      "\n",
      "* Ein Lungenfunktionstest, um zu sehen, wie gut die Lungen arbeiten\n",
      "* Ein Allergietest, um zu sehen, ob bestimmte Dinge die Asthma-Symptome auslösen\n",
      "* Ein Untersuchung der Atemwege, um zu sehen, ob sie eng oder geschwollen sind\n",
      "\n",
      "Der Arzt oder die Ärztin wird auch Fragen stellen, wie zum Beispiel:\n",
      "\n",
      "* Wann haben die Symptome begonnen?\n",
      "* Wie oft treten sie auf?\n",
      "* Was kann sie auslösen?\n",
      "\n",
      "All diese Informationen helfen dem Arzt oder der Ärztin, zu bestimmen, ob jemand Asthma hat und wie es behandelt werden kann.\n",
      "\n",
      "Also, der wichtigste Faktor bei der Diagnostizierung von Asthma ist, wie jemand atmet und wie seine Atemwege reagieren. Es ist wichtig, dass man mit einem Arzt oder einer Ärztin spricht, wenn man Symptome hat, die auf Asthma hindeuten könnten.\n",
      "Rewritten Query: Welche diagnostischen Kriterien oder Faktoren sind in den Leitlinien zur Asthma-Diagnostik als entscheidend oder wichtig definiert?\n",
      "Hallo kleiner Freund! Wenn wir über Asthma sprechen, ist es wichtig, einige Dinge zu wissen.\n",
      "\n",
      "Ein wichtiger Faktor bei der Diagnostizierung von Asthma ist die Art, wie du atmest. Wenn du Asthma hast, können deine Atemwege eng werden und es fällt dir schwer, Luft zu holen. Das kann zu Husten, Keuchen oder Atemnot führen.\n",
      "\n",
      "Ein Arzt kann verschiedene Tests durchführen, um herauszufinden, ob du Asthma hast. Zum Beispiel kann er deine Lungenfunktion überprüfen, indem er dich bittest, in ein Gerät zu blasen, das deine Atemfähigkeit misst.\n",
      "\n",
      "Aber der wichtigste Faktor ist eigentlich, wie deine Atemwege reagieren, wenn sie bestimmte Dinge ausgesetzt sind, wie zum Beispiel Allergene oder Rauch. Wenn deine Atemwege sehr empfindlich sind und leicht reagieren, kann das ein Zeichen für Asthma sein.\n",
      "\n",
      "In dem Text, den du mir gegeben hast, steht, dass bestimmte Medikamente nicht gut für Menschen mit schwerem Asthma sind. Das ist wichtig, weil diese Medikamente die Atemwege noch weiter verengen können und es noch schwerer machen, Luft zu holen.\n",
      "\n",
      "Also, der wichtigste Faktor bei der Diagnostizierung von Asthma ist, wie deine Atemwege reagieren und wie du atmest. Ein Arzt kann helfen, diese Dinge zu überprüfen und herauszufinden, ob du Asthma hast oder nicht.\n",
      "Rewritten Query: Bei der Diagnostizierung von Asthma: Welche klinischen Anzeichen und Symptome sollten als wichtigster Faktor berücksichtigt werden?\n",
      "Hallo kleiner Freund! Wir sprechen gerade über Asthma. Du weißt, wenn du Schwierigkeiten hast, zu atmen, und es dir schwerfällt, Luft zu bekommen? Das ist Asthma.\n",
      "\n",
      "Der wichtigste Faktor bei der Diagnostizierung von Asthma ist... (dramatische Pause) ...die Anamnese! Das bedeutet, dass der Arzt mit dir und deiner Familie spricht, um herauszufinden, ob du irgendwelche Symptome wie Atemnot, Husten oder Keuchen hast. Er wird auch fragen, ob du bereits in der Vergangenheit ähnliche Symptome hattest.\n",
      "\n",
      "Der Arzt wird auch einige Tests durchführen, wie zum Beispiel einen Lungenfunktions-Test, um zu sehen, wie gut deine Lungen funktionieren. Aber die Anamnese ist der wichtigste Teil, um herauszufinden, ob du Asthma hast.\n",
      "\n",
      "Es ist wie ein Puzzle, kleiner Freund! Der Arzt sammelt alle Informationen, um das Bild von deiner Gesundheit zu vervollständigen. Und wenn er denkt, dass du Asthma hast, wird er dir helfen, es zu behandeln, damit du wieder frei atmen kannst!\n",
      "Rewritten Query: Was sind die diagnostischen Kriterien für Asthma und welchen Stellenwert hat die klinische Symptomatik bei der Feststellung einer Asthma-Diagnose?\n",
      "Das ist eine gute Frage!\n",
      "\n",
      "Also, wenn wir über Asthma sprechen, ist es sehr wichtig, den Atemweg zu überprüfen. Der wichtigste Faktor bei der Diagnostizierung von Asthma ist, wie der Arzt den Atemweg und die Lungen überprüft.\n",
      "\n",
      "Stell dir vor, du hast ein Rohr, das Luft in deine Lungen bringt. Wenn das Rohr eng oder blockiert ist, kann die Luft nicht richtig durchkommen. Das ist ein bisschen wie Asthma. Der Arzt muss überprüfen, ob das Rohr (der Atemweg) frei ist oder nicht.\n",
      "\n",
      "Um das zu überprüfen, macht der Arzt oft einen Test, der \"Bronchospasmen\"-Test genannt wird. Das ist ein bisschen wie ein Spiel, bei dem der Arzt sieht, wie gut du atmen kannst. Wenn du Asthma hast, kann es sein, dass deine Atemwege eng sind und du nicht so gut atmen kannst.\n",
      "\n",
      "Also, der wichtigste Faktor bei der Diagnostizierung von Asthma ist, wie der Arzt den Atemweg und die Lungen überprüft, um zu sehen, ob du Asthma hast oder nicht. Verstehst du?\n",
      "Rewritten Query: Für die Diagnostizierung von Asthma: Welche klinischen Kriterien oder Symptome sollten als primäre Diagnosekriterien berücksichtigt werden?\n",
      "Hallo kleiner Freund! Asthma ist eine Sache, die mit unserer Atmung zu tun hat. Wenn wir Asthma haben, kann es uns manchmal schwer fallen, Luft in unsere Lungen zu bekommen.\n",
      "\n",
      "Der wichtigste Faktor bei der Diagnostizierung von Asthma sind die Symptome, die wir haben. Das bedeutet, dass der Arzt uns Fragen stellt, wie zum Beispiel:\n",
      "\n",
      "* Hast du jemals Schwierigkeiten beim Atmen gehabt?\n",
      "* Bekommst du manchmal Husten oder Keuchen?\n",
      "* Fühlst du dich manchmal muffig oder haben deine Brustwehwehgetan?\n",
      "\n",
      "Außerdem kann der Arzt uns auch ein paar Tests machen, wie zum Beispiel:\n",
      "\n",
      "* Ein Lungentest, um zu sehen, wie gut wir Luft in unsere Lungen bekommen können.\n",
      "* Ein Bluttest, um zu sehen, ob wir allergisch gegen bestimmte Dinge sind.\n",
      "\n",
      "Wenn wir alle diese Informationen zusammennehmen, kann der Arzt dann sagen, ob wir Asthma haben oder nicht. Es ist wichtig, dass wir ehrlich sind und dem Arzt alles erzählen, damit er uns helfen kann, uns besser zu fühlen.\n",
      "\n",
      "Es gibt auch bestimmte Dinge, die den Arzt auf Asthma hinweisen können, wie zum Beispiel:\n",
      "\n",
      "* Wenn wir oft husten oder keuchen\n",
      "* Wenn wir Schwierigkeiten beim Atmen haben\n",
      "* Wenn wir allergisch gegen bestimmte Dinge sind\n",
      "\n",
      "Das sind alles wichtige Hinweise, die uns helfen können, Asthma zu diagnostizieren. Aber der wichtigste Faktor ist wirklich, dass wir ehrlich sind und dem Arzt alles erzählen.\n"
     ]
    }
   ],
   "source": [
    "# Beantwortung der 10 generierten Fragen\n",
    "\n",
    "for question in questions:  # Questions list from Aufgabe (7)\n",
    "    \n",
    "    # Use the RAG chain to get an answer for the question\n",
    "    answer =  answer_query_with_rewriting(query, 4, index, chunks, groq_key)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e8e4d8",
   "metadata": {},
   "source": [
    "#### <b> TASK (9) Your assessment of the quality (double-click to edit the cell below):</b>\n",
    "\n",
    "- Briefly describe what seems to work well in your RAG pipeline based on the answers to the 10 generated questions above.\n",
    "- Give at least one example of a question/answer pair that worked particularly well.\n",
    "- Point out at least one aspect or example where the pipeline could be improved or did not work as expected.\n",
    "\n",
    "<b style=\"color: gray;\">(max. achievable points: 2)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc97c26",
   "metadata": {},
   "source": [
    "I think everything in my project is working very well. The RAG pipeline is performing nicely and there are no errors when executing. Sometimes I maybe had to do an other solution than wished, for example at \"printing the questions\" but still it worked perfectly. Maybe I could have used a german model, I used the multilingual one. Most of the parts worked extreamly well, for example task 6. I think I could improve task 7 if needed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff525b1e",
   "metadata": {},
   "source": [
    "### Jupyter notebook --footer info-- (please always provide this at the end of each notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e6cae5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "POSIX\n",
      "Linux | 6.8.0-1027-azure\n",
      "Datetime: 2025-05-20 09:07:48\n",
      "Python Version: 3.12.1\n",
      "IP Address: 127.0.0.1\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import socket\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('IP Address:', socket.gethostbyname(socket.gethostname()))\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
